{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHLu0NqGY2Nb",
        "outputId": "083a077a-f5c1-4862-d9ff-b2e5b76fb726"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/TransNAS_TSAD"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ujjix88_Y27Y",
        "outputId": "25b8c6f7-2ace-4b4a-84c2-cc29f04b1189"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/TransNAS_TSAD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n"
      ],
      "metadata": {
        "id": "y6qhVDElaHtV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = '/content/drive/My Drive/TransNAS_TSAD'  # Replace with your actual path\n",
        "OUTPUT_FOLDER = os.path.join(DATA_PATH, 'Pre_processed_data')\n",
        "#CHECKPOINT_FOLDER = os.path.join(DATA_PATH, 'checkpoints')"
      ],
      "metadata": {
        "id": "sqiBXviGYyVT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.dataset = 'SMAP'  # Replace with your dataset name.\n",
        "\n",
        "\n",
        "config = Config()"
      ],
      "metadata": {
        "id": "SeMEwMNkYhdl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4m6cQbnvYWui"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import json\n",
        "from shutil import copyfile\n",
        "\n",
        "\n",
        "datasets = ['SMD', 'SWaT', 'SMAP', 'MSL', 'WADI', 'UCR', 'MBA', 'NAB']\n",
        "\n",
        "wadi_drop = ['2_LS_001_AL', '2_LS_002_AL','2_P_001_STATUS','2_P_002_STATUS']\n",
        "\n",
        "def load_and_save(category, filename, dataset, dataset_folder):\n",
        "    temp = np.genfromtxt(os.path.join(dataset_folder, category, filename),\n",
        "                         dtype=np.float64,\n",
        "                         delimiter=',')\n",
        "    print(dataset, category, filename, temp.shape)\n",
        "    np.save(os.path.join(OUTPUT_FOLDER, f\"SMD/{dataset}_{category}.npy\"), temp)\n",
        "    return temp.shape\n",
        "\n",
        "def load_and_save2(category, filename, dataset, dataset_folder, shape):\n",
        "\ttemp = np.zeros(shape)\n",
        "\twith open(os.path.join(dataset_folder, 'interpretation_label', filename), \"r\") as f:\n",
        "\t\tls = f.readlines()\n",
        "\tfor line in ls:\n",
        "\t\tpos, values = line.split(':')[0], line.split(':')[1].split(',')\n",
        "\t\tstart, end, indx = int(pos.split('-')[0]), int(pos.split('-')[1]), [int(i)-1 for i in values]\n",
        "\t\ttemp[start-1:end-1, indx] = 1\n",
        "\tprint(dataset, category, filename, temp.shape)\n",
        "\tnp.save(os.path.join(OUTPUT_FOLDER, f\"SMD/{dataset}_{category}.npy\"), temp)\n",
        "\n",
        "def normalize(a):\n",
        "\ta = a / np.maximum(np.absolute(a.max(axis=0)), np.absolute(a.min(axis=0)))\n",
        "\treturn (a / 2 + 0.5)\n",
        "\n",
        "def normalize2(a, min_a = None, max_a = None):\n",
        "\tif min_a is None: min_a, max_a = min(a), max(a)\n",
        "\treturn (a - min_a) / (max_a - min_a), min_a, max_a\n",
        "\n",
        "def normalize3(a, min_a = None, max_a = None):\n",
        "\tif min_a is None: min_a, max_a = np.min(a, axis = 0), np.max(a, axis = 0)\n",
        "\treturn (a - min_a) / (max_a - min_a + 0.0001), min_a, max_a\n",
        "\n",
        "def convertNumpy(df):\n",
        "\tx = df[df.columns[3:]].values[::10, :]\n",
        "\treturn (x - x.min(0)) / (x.ptp(0) + 1e-4)\n",
        "\n",
        "def load_data(dataset):\n",
        "\tfolder = os.path.join(OUTPUT_FOLDER, dataset)\n",
        "\tdata_folder='data'\n",
        "\tos.makedirs(folder, exist_ok=True)\n",
        "\n",
        "\tif dataset == 'SMD':\n",
        "\t\tdataset_folder = 'data/SMD'\n",
        "\t\tfile_list = os.listdir(os.path.join(dataset_folder, \"train\"))\n",
        "\t\tfor filename in file_list:\n",
        "\t\t\tif filename.endswith('.txt'):\n",
        "\t\t\t\tload_and_save('train', filename, filename.strip('.txt'), dataset_folder)\n",
        "\t\t\t\ts = load_and_save('test', filename, filename.strip('.txt'), dataset_folder)\n",
        "\t\t\t\tload_and_save2('labels', filename, filename.strip('.txt'), dataset_folder, s)\n",
        "\n",
        "\telif dataset == 'UCR':\n",
        "\t\tdataset_folder = 'data/UCR'\n",
        "\t\tfile_list = os.listdir(dataset_folder)\n",
        "\t\tfor filename in file_list:\n",
        "\t\t\tif not filename.endswith('.txt'): continue\n",
        "\t\t\tvals = filename.split('.')[0].split('_')\n",
        "\t\t\tdnum, vals = int(vals[0]), vals[-3:]\n",
        "\t\t\tvals = [int(i) for i in vals]\n",
        "\t\t\ttemp = np.genfromtxt(os.path.join(dataset_folder, filename),\n",
        "\t\t\t\t\t\t\t\tdtype=np.float64,\n",
        "\t\t\t\t\t\t\t\tdelimiter=',')\n",
        "\t\t\tmin_temp, max_temp = np.min(temp), np.max(temp)\n",
        "\t\t\ttemp = (temp - min_temp) / (max_temp - min_temp)\n",
        "\t\t\ttrain, test = temp[:vals[0]], temp[vals[0]:]\n",
        "\t\t\tlabels = np.zeros_like(test)\n",
        "\t\t\tlabels[vals[1]-vals[0]:vals[2]-vals[0]] = 1\n",
        "\t\t\ttrain, test, labels = train.reshape(-1, 1), test.reshape(-1, 1), labels.reshape(-1, 1)\n",
        "\t\t\tfor file in ['train', 'test', 'labels']:\n",
        "\t\t\t\tnp.save(os.path.join(folder, f'{dnum}_{file}.npy'), eval(file))\n",
        "\n",
        "\telif dataset == 'NAB':\n",
        "\t\tdataset_folder = 'data/NAB'\n",
        "\t\tfile_list = os.listdir(dataset_folder)\n",
        "\t\twith open(dataset_folder + '/labels.json') as f:\n",
        "\t\t\tlabeldict = json.load(f)\n",
        "\t\tfor filename in file_list:\n",
        "\t\t\tif not filename.endswith('.csv'): continue\n",
        "\t\t\tdf = pd.read_csv(dataset_folder+'/'+filename)\n",
        "\t\t\tvals = df.values[:,1]\n",
        "\t\t\tlabels = np.zeros_like(vals, dtype=np.float64)\n",
        "\t\t\tfor timestamp in labeldict['realKnownCause/'+filename]:\n",
        "\t\t\t\ttstamp = timestamp.replace('.000000', '')\n",
        "\t\t\t\tindex = np.where(((df['timestamp'] == tstamp).values + 0) == 1)[0][0]\n",
        "\t\t\t\tlabels[index-4:index+4] = 1\n",
        "\t\t\tmin_temp, max_temp = np.min(vals), np.max(vals)\n",
        "\t\t\tvals = (vals - min_temp) / (max_temp - min_temp)\n",
        "\t\t\ttrain, test = vals.astype(float), vals.astype(float)\n",
        "\t\t\ttrain, test, labels = train.reshape(-1, 1), test.reshape(-1, 1), labels.reshape(-1, 1)\n",
        "\t\t\tfn = filename.replace('.csv', '')\n",
        "\t\t\tfor file in ['train', 'test', 'labels']:\n",
        "\t\t\t\tnp.save(os.path.join(folder, f'{fn}_{file}.npy'), eval(file))\n",
        "\n",
        "\telif dataset == 'SWaT':\n",
        "\t\tdataset_folder = 'data/SWaT'\n",
        "\t\tfile = os.path.join(dataset_folder, 'series.json')\n",
        "\t\tdf_train = pd.read_json(file, lines=True)[['val']][3000:6000]\n",
        "\t\tdf_test  = pd.read_json(file, lines=True)[['val']][7000:12000]\n",
        "\t\ttrain, min_a, max_a = normalize2(df_train.values)\n",
        "\t\ttest, _, _ = normalize2(df_test.values, min_a, max_a)\n",
        "\t\tlabels = pd.read_json(file, lines=True)[['noti']][7000:12000] + 0\n",
        "\t\tfor file in ['train', 'test', 'labels']:\n",
        "\t\t\tnp.save(os.path.join(folder, f'{file}.npy'), eval(file))\n",
        "\n",
        "\telif dataset in ['SMAP', 'MSL']:\n",
        "\t\tdataset_folder = 'data/SMAP_MSL'\n",
        "\t\tfile = os.path.join(dataset_folder, 'labeled_anomalies.csv')\n",
        "\t\tvalues = pd.read_csv(file)\n",
        "\t\tvalues = values[values['spacecraft'] == dataset]\n",
        "\t\tfilenames = values['chan_id'].values.tolist()\n",
        "\t\tfor fn in filenames:\n",
        "\t\t\ttrain = np.load(f'{dataset_folder}/train/{fn}.npy')\n",
        "\t\t\ttest = np.load(f'{dataset_folder}/test/{fn}.npy')\n",
        "\t\t\ttrain, min_a, max_a = normalize3(train)\n",
        "\t\t\ttest, _, _ = normalize3(test, min_a, max_a)\n",
        "\t\t\tnp.save(f'{folder}/{fn}_train.npy', train)\n",
        "\t\t\tnp.save(f'{folder}/{fn}_test.npy', test)\n",
        "\t\t\tlabels = np.zeros(test.shape)\n",
        "\t\t\tindices = values[values['chan_id'] == fn]['anomaly_sequences'].values[0]\n",
        "\t\t\tindices = indices.replace(']', '').replace('[', '').split(', ')\n",
        "\t\t\tindices = [int(i) for i in indices]\n",
        "\t\t\tfor i in range(0, len(indices), 2):\n",
        "\t\t\t\tlabels[indices[i]:indices[i+1], :] = 1\n",
        "\t\t\tnp.save(f'{folder}/{fn}_labels.npy', labels)\n",
        "\n",
        "\telif dataset == 'WADI':\n",
        "\t\tdataset_folder = 'data/WADI'\n",
        "\t\tls = pd.read_csv(os.path.join(dataset_folder, 'WADI_attacklabels.csv'))\n",
        "\t\ttrain = pd.read_csv(os.path.join(dataset_folder, 'WADI_14days.csv'), skiprows=1000, nrows=2e5)\n",
        "\t\ttest = pd.read_csv(os.path.join(dataset_folder, 'WADI_attackdata.csv'))\n",
        "\t\ttrain.dropna(how='all', inplace=True); test.dropna(how='all', inplace=True)\n",
        "\t\ttrain.fillna(0, inplace=True); test.fillna(0, inplace=True)\n",
        "\t\ttest['Time'] = test['Time'].astype(str)\n",
        "\t\ttest['Time'] = pd.to_datetime(test['Date'] + ' ' + test['Time'])\n",
        "\t\tlabels = test.copy(deep = True)\n",
        "\t\tfor i in test.columns.tolist()[3:]: labels[i] = 0\n",
        "\t\tfor i in ['Start Time', 'End Time']:\n",
        "\t\t\tls[i] = ls[i].astype(str)\n",
        "\t\t\tls[i] = pd.to_datetime(ls['Date'] + ' ' + ls[i])\n",
        "\t\tfor index, row in ls.iterrows():\n",
        "\t\t\tto_match = row['Affected'].split(', ')\n",
        "\t\t\tmatched = []\n",
        "\t\t\tfor i in test.columns.tolist()[3:]:\n",
        "\t\t\t\tfor tm in to_match:\n",
        "\t\t\t\t\tif tm in i:\n",
        "\t\t\t\t\t\tmatched.append(i); break\n",
        "\t\t\tst, et = str(row['Start Time']), str(row['End Time'])\n",
        "\t\t\tlabels.loc[(labels['Time'] >= st) & (labels['Time'] <= et), matched] = 1\n",
        "\t\ttrain, test, labels = convertNumpy(train), convertNumpy(test), convertNumpy(labels)\n",
        "\t\tprint(train.shape, test.shape, labels.shape)\n",
        "\t\tfor file in ['train', 'test', 'labels']:\n",
        "\t\t\tnp.save(os.path.join(folder, f'{file}.npy'), eval(file))\n",
        "\n",
        "\telif dataset == 'MBA':\n",
        "\t\tdataset_folder = 'data/MBA'\n",
        "\t\tls = pd.read_excel(os.path.join(dataset_folder, 'labels.xlsx'))\n",
        "\t\ttrain = pd.read_excel(os.path.join(dataset_folder, 'train.xlsx'))\n",
        "\t\ttest = pd.read_excel(os.path.join(dataset_folder, 'test.xlsx'))\n",
        "\t\ttrain, test = train.values[1:,1:].astype(float), test.values[1:,1:].astype(float)\n",
        "\t\ttrain, min_a, max_a = normalize3(train)\n",
        "\t\ttest, _, _ = normalize3(test, min_a, max_a)\n",
        "\t\tls = ls.values[:,1].astype(int)\n",
        "\t\tlabels = np.zeros_like(test)\n",
        "\t\tfor i in range(-20, 20):\n",
        "\t\t\tlabels[ls + i, :] = 1\n",
        "\t\tfor file in ['train', 'test', 'labels']:\n",
        "\t\t\tnp.save(os.path.join(folder, f'{file}.npy'), eval(file))\n",
        "\telse:\n",
        "\t\traise Exception(f'Not Implemented. Check one of {datasets}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    dataset_name = config.dataset  # Assuming config.dataset is defined\n",
        "    load_data(dataset_name)"
      ],
      "metadata": {
        "id": "8Btcj0ZzaoXB"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}